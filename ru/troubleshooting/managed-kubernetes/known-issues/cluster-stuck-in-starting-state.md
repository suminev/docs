# Кластер слишком долго находится в состоянии «Starting»


## Описание проблемы {#issue-description}

* При попытке создать или запустить существующий кластер {{ managed-k8s-name }} кластер переходит в состояние `STARTING`, но спустя длительное время не переходит в состояние `RUNNING`.
* Кластер завис в состоянии `STARTING`.
* Состояние кластера {{ managed-k8s-name }} не изменяется на `RUNNING`


## Решение {#issue-resolution}

Одной из причин того, что операция создания или запуска кластера {{ managed-k8s-name }} «застревает» в процессе выполнения, может быть превышение одной или нескольких квот из этого списка:

* Количество vCPU виртуальных машин;
* Общий объем RAM виртуальных машин;
* Количество дисков;
* Общий объем HDD-дисков;
* Общий объем SSD-дисков;
* Количество кластеров Kubernetes;
* Количество групп узлов;
* Количество узлов;
* Суммарное количество vCPU для всех узлов;
* Суммарный объем RAM;
* Суммарный объем дисков.

Выполните следующие действия для решения проблемы:

1. Проверьте потребление по квотам из указанного выше списка. Если потребление по одной или нескольким из этих квот превышено либо близко к превышению, операции создания новых ресурсов могут завершаться с ошибкой. Выяснить текущий уровень использования квот и запросить их повышение вы можете [на этой странице]({{ link-console-quotas }}).
2. Проверьте, привязан ли к кластеру {{ managed-k8s-name }} сервисный аккаунт и имеет ли он необходимые роли:
   * `k8s.clusters.agent` – обязательна для всех сервисных аккаунтов, привязанных ко всем кластерам {{ managed-k8s-name }};
   * `vpc.publicAdmin` – обязательна для сервисных аккаунтов, привязанных к кластерам {{ managed-k8s-name }}, мастер-нодам или воркер-нодам которых сопоставлены публичные IP-адреса.

## Если проблема осталась {#if-issue-still-persists}

Если вышеописанные действия не помогли решить проблему, [создайте запрос в техническую поддержку]({{ link-console-support }}).
При создании запроса просим указать идентификатор проблемного кластера {{ managed-k8s-name }}.
